#!/usr/bin/env python3

# Copyright ARDUINO SRL (https://www.arduino.cc)
# SPDX-License-Identifier: Apache-2.0

# Script to analyze CI build and test logs, summarize results,
# and generate a markdown report.
#
# The script reads build artifacts and test reports generated during a GitHub
# Actions CI run for Arduino Zephyr cores. It aggregates results by board,
# artifact, and sketch, reports failures and warnings, and produces a
# comprehensive Github-friendly Markdown report.
#
# This scripts expects the following inputs:
#
#  - Environment variables:
#     - ALL_BOARD_DATA: JSON array of all board metadata, generated by get_board_details.sh
#     - WORKFLOW_JOBS: JSON array of job names and URLs, generated by workflow YAML
#     - GITHUB_REPOSITORY, GITHUB_RUN_ID: workflow context variables for writing links
#
#  - Files in the current directory:
#     - zephyr-<variant>.config: Kconfig settings, generated at loader build time
#     - zephyr-<variant>.warnings: Loader compilation warnings
#     - zephyr-<variant>.meminfo: Loader memory usage report
#     - arduino-<subarch>-<board>-link_mode=<mode>.json: Compilation test report files
#     - variants/<variant>/known_example_issues.txt: Expected errors for that variant
#
# This script produces three output files:
#
#  - <results_file>: simple text file with "PASSED" in it if all tests passed, or an error message
#  - <summary_file>: markdown summary report with overall results
#  - <full_report_file>: full markdown report with detailed logs
#

from collections import defaultdict
import json
import os
import re
import sys

# Constants for test statuses, in increasing order of severity
SKIP = -1           # Test was not performed
PASS = 0            # (PASS)  Compiled successfully
WARNING = 1         # (PASS)  Compiled with warnings
ERROR = 2           # (FAIL)  Compilation failed with errors
FAILURE = 3         # Test run failed to complete

# Status legends and icons, indexed by status constant
TEST_LEGEND = [
        "Test passed successfully, with no warnings or errors.",
        "Test completed with some warnings; no errors detected.",
        "Test completed with unexpected errors.",
        "Test run failed to complete.",
        "Test was skipped." # -1
]

TEST_STATUS = [
    ":green_circle:",
    ":yellow_circle:",
    ":red_circle:",
    ":fire:",
    ":new_moon:" # -1
]

BOARD_STATUS = [
    ":white_check_mark:",
    ":white_check_mark:*",
    ":x:",
    ":fire:",
    ":new_moon:" # -1
]

# Loader build status data structure, one per loader
class LoaderEntry:
    def __init__(self, artifact, board, variant, job_link):
        self.artifact = artifact
        self.board = board
        self.variant = variant
        self.job_link = job_link

        self.status = PASS

# Single test data structure, one per compilation test
class TestEntry:
    def __init__(self, artifact, board, sketch, status, issues, job_link):
        self.artifact = artifact
        self.board = board
        self.sketch = sketch
        self.status = status
        self.issues = issues
        self.job_link = job_link

        # Extract group and name from sketch path:
        #                   (1.................) (2....) (3................) (4.)
        match = re.search(r'(libraries|examples)/([^/]+)/(examples/|extras/)?(.*)', sketch)
        if match:
            self.group = match.group(2)
            self.name = match.group(4)
        else:
            self.group = ""
            self.name = sketch

# Summary data structure, used to group test results
class TestGroup:
    def __init__(self):
        # Sets to track unique board, sketch and (group,name) tuples
        self.boards = set()
        self.sketches = set()
        self.group_names = set()
        # Counts of test results by status
        self.counts = { status : 0 for status in [PASS, WARNING, ERROR, FAILURE] }
        # Overall status of the group
        self.status = SKIP
        # List of individual TestEntry objects (all, only with issues)
        self.tests = []
        self.tests_with_issues = []

    def track(self, test_entry):
        """
        Update this group with a new test entry
        """
        self.tests.append(test_entry)
        if test_entry.issues:
            self.tests_with_issues.append(test_entry)

        self.counts[test_entry.status] += 1
        self.status = max(self.status, test_entry.status)
        self.boards.add(test_entry.board)
        self.sketches.add(test_entry.sketch)
        self.group_names.add((test_entry.group, test_entry.name))

# Global Data Structures
# ----------------------

# Loader build results, one per board
BOARD_LOADERS = {}                                          # { board: LoaderEntry() }

# Test results grouped by artifact, board, and artifact/sketch
# (grouping sketch results across different artifacts is really confusing)
ARTIFACT_TESTS = defaultdict(TestGroup)                     # { artifact: TestGroup() }
BOARD_TESTS = defaultdict(TestGroup)                        # { board: TestGroup() }
SKETCH_TESTS = defaultdict(lambda: defaultdict(TestGroup))  # { artifact: { sketch: TestGroup() } }

def log_test(artifact, board, sketch, status, issues, job_link=None):
    """
    Logs individual test results into the global test tracking structures.
    """

    # Ensure issues is a list
    if isinstance(issues, str):
        issues = [ issues ]

    # Create the test entry
    test_entry = TestEntry(artifact, board, sketch, status, issues, job_link)

    # Track in global structures
    ARTIFACT_TESTS[artifact].track(test_entry)
    BOARD_TESTS[board].track(test_entry)
    SKETCH_TESTS[artifact][sketch].track(test_entry)

def print_summary():
    """
    Prints the summary section of the report, including overall status and a recap table.
    """

    # Title
    if ci_run_passed:
        title = f"# [CI run]({JOB_LINK_STEM}#user-content-summary) PASSED :green_circle:\n"
    else:
        failed_boards = [ f"{BOARD_STATUS[res.status]} `{board}`" for board, res in BOARD_TESTS.items() if res.status in (ERROR, FAILURE) ]
        title = f"# [CI run]({JOB_LINK_STEM}#user-content-summary) FAILED: {', '.join(failed_boards)}\n"
    f_print("<a name='summary'></a>\n")
    f_print(title)

    # Print the recap table, one line per board.
    # 8 columns:
    # - Artifact name (multi-row for boards under the same artifact)
    # - Board name
    # - Core compilation status (ok, number of warnings)
    # - Overall sketch compilation status for the core
    # - Used RAM percent
    # - Sketches tested
    # - Sketches with warnings
    # - Failed sketches (and ignored errors)
    f_print("<table>\n<tr><th>Artifact</th><th>Board</th><th>Core</th><th>Tests</th><th>RAM</th><th>Sketches</th><th>Warnings</th><th>Errors</th></tr>")

    for artifact in ARTIFACTS:
        artifact_boards = sorted(ARTIFACT_TESTS[artifact].boards)
        artifact_status = ARTIFACT_TESTS[artifact].status

        first_row_text = f"<td rowspan='{len(artifact_boards)}'>{BOARD_STATUS[artifact_status]} <a href='{JOB_LINK_STEM}#user-content-{artifact}'><code>{artifact}</code></a></td>"
        for board in artifact_boards:
            # Artifact name (multi-row)
            f_print(f"<tr>{first_row_text}")
            first_row_text = ""

            # Board name
            res = BOARD_LOADERS[board]
            if res.job_link:
                f_print(f"<td><code><a href='{res.job_link}'>{board}</a></code>")
            else:
                f_print(f"<td><code>{board}</code>")

            # Core build status + message on failure
            if res.status == FAILURE:
                f_print(f"<td align='center'>{BOARD_STATUS[FAILURE]}</td><td colspan='5'>Core build failed!</td></tr>")
                continue

            pin = f"{len(res.warnings)} :label:" if res.status == WARNING else ":green_book:"
            f_print(f"</td><td align='center'>{pin}</td>")

            # Sketch build status + message on failure
            res = BOARD_TESTS[board]
            f_print(f"<td align='center'>{BOARD_STATUS[res.status]}</td>")
            if res.status == FAILURE:
                f_print(f"<td colspan='4'>")
                f_print("<br>".join(f"{test.issues[0]} (<a href='{test.job_link}'>full log</a>)" for test in res.tests))
                f_print("</td></tr>")
                continue

            # Test count summary
            tests_str = len(res.tests) or "-"
            warnings_str = res.counts[WARNING] or "-"
            errors_str = f"<b>{res.counts[ERROR]}</b>" if res.counts[ERROR] else "-"
            f_print(f"<td align='right'>{tests_str}</td><td align='right'>{warnings_str}</td><td align='right'>{errors_str}</td></tr>")
    f_print("</table>\n")

    # Print the legend
    f_print("<details><summary>Legend</summary>")
    f_print("<blockquote><br><table><tr><th align='center'>Board</th><th align='center'>Test</th><th>Status description</th></tr>")
    for status in FAILURE, ERROR, WARNING, PASS, SKIP:
        f_print(f"<tr><td align='center'>{BOARD_STATUS[status]}</td>")
        f_print(f"<td align='center'>{TEST_STATUS[status]}</td>")
        f_print(f"<td>{TEST_LEGEND[status]}</td></tr>")
    f_print("</table></blockquote></details>\n")

    # Print artifact error warnings
    for artifact in ARTIFACTS:
        artifact_boards = sorted(ARTIFACT_TESTS[artifact].boards)
        failed_boards = [ f"`{board}`" for board in artifact_boards if BOARD_TESTS[board].status in (ERROR, FAILURE) ]

        if failed_boards:
            f_print("> [!CAUTION]")
            f_print(f"> `{artifact}` is blocked due to failures on {', '.join(failed_boards)}!\n")

def print_test_matrix(artifact, artifact_boards, title, sketch_filter=lambda x: True):
    """
    Prints a matrix of test results for a given artifact and its boards. The
    sketch_filter function determines which sketches to include. No table is printed
    if no sketches pass the filter.
    """

    # Build the header row, which includes board names and board statuses.
    # Headers have a link to the board's CI job, if available.
    header_row = f"<tr><th colspan='2'><code>{artifact}</code> {title}</th>"
    for board in artifact_boards:
        res = BOARD_TESTS[board]
        header_col = f"<code>{board}</code><br>{BOARD_STATUS[res.status]}"
        header_row += f"<th>{header_col}</th>"
    header_row += "</tr>"

    # Group sketches by library
    sketch_groups = defaultdict(list)
    for sketch in ARTIFACT_TESTS[artifact].sketches:
        res = SKETCH_TESTS[artifact][sketch]
        if not sketch_filter(res):
            continue

        for group, name in res.group_names:
            sketch_groups[group].append((name, res))

    # Build the data rows, grouping libraries together. Each row corresponds to
    # a sketch, each cell to the test result icon of that sketch on that board.
    data_rows = []
    for group in sorted(sketch_groups.keys()):
        if group:
            data_rows.append(f"<tr><th colspan='2' align='left'><code>{group}</code></th><th colspan={len(artifact_boards)}><code>{group}</code></th></tr>")
            #data_rows.append(f"<tr><th colspan='2' align='left'><code>{group}</code></th>{''.join('<td align=center>---</td>' for x in artifact_boards)}</tr>")
        for sample, res in sorted(sketch_groups[group]):
            row_data = f"<tr><td>{TEST_STATUS[res.status]}</td>"
            # If there are issues, make the sketch name a link to the detailed logs below
            name_link = f"<code>{sample}</code>"
            if res.tests_with_issues:
                sketch = next(iter(res.sketches)) # only one
                sketch_id = sketch.replace('/', '_').replace(' ', '_').replace('-', '_')
                name_link = f"<a href='{JOB_LINK_STEM}#user-content-{artifact}_{sketch_id}'>{name_link}</a>"
            row_data += f"<td>{name_link}</td>"

            for board in artifact_boards:
                test = next((test for test in res.tests if test.board == board), None)
                status = test.status if test else SKIP
                issues = test.issues if test else ""

                row_data += f"<td align='center'>{TEST_STATUS[status]}</td>"

            row_data += "</tr>"
            data_rows.append(row_data)

    if not data_rows:
        return

    # Print the table
    f_print("<table>")
    f_print(header_row)
    for row in data_rows:
        f_print(row)
    f_print("</table>\n")

    # Print detailed logs for sketches with issues
    for group in sorted(sketch_groups.keys()):
        for sample, res in sorted(sketch_groups[group]):
            if not res.tests_with_issues:
                continue

            # Header and anchor for the detailed logs
            sketch = next(iter(res.sketches)) # only one
            sketch_id = sketch.replace('/', '_').replace(' ', '_').replace('-', '_')
            f_print(f"<a name='{artifact}_{sketch_id}'></a>")
            f_print(f"<details name='{artifact}_{title}'><summary><code>{artifact}</code> logs for {TEST_STATUS[res.status]} <code>{group}</code> <code>{sample}</code></summary>")
            f_print("<blockquote><br><table>")

            # Test logs by board, group similar messages
            boards_by_issues = defaultdict(list)   # { issues: list of boards }
            for test in sorted(res.tests_with_issues, key=lambda x: x.status, reverse=True):
                test_text = f"<code>{test.board}</code>"
                if test.job_link:
                    test_text += f" (<a href='{test.job_link}'>full log</a>)"
                test_text = f"{TEST_STATUS[test.status]} {test_text}"

                boards_by_issues[tuple(test.issues)].append(test_text)

            for issues, board_texts in boards_by_issues.items():
                f_print(f"<tr><td>{', '.join(board_texts)}<br>\n\n```")
                for line in issues:
                    f_print(line)
                f_print("```\n</td></tr>")

            f_print("</table></blockquote></details>\n")




# ---------------------------------------------------------------------------
# Main Logic
# ---------------------------------------------------------------------------

# Environment Variable Checks
ALL_BOARD_DATA_STR = os.environ.get('ALL_BOARD_DATA')
WORKFLOW_JOBS_STR = os.environ.get('WORKFLOW_JOBS')
GITHUB_REPOSITORY = os.environ.get('GITHUB_REPOSITORY')
GITHUB_RUN_ID = os.environ.get('GITHUB_RUN_ID')
JOB_LINK_STEM = f"https://github.com/{GITHUB_REPOSITORY}/actions/runs/{GITHUB_RUN_ID}"

if not ALL_BOARD_DATA_STR or not GITHUB_REPOSITORY or not GITHUB_RUN_ID:
    print("Not in a Github CI run, cannot proceed.")
    sys.exit(0)

if not len(sys.argv) in (1, 4):
    print("Usage: ci_inspect_logs.py [<results_file> <summary_file> <full_report_file>]")
    sys.exit(1)

if len(sys.argv) == 4:
    results_file = sys.argv[1]
    summary_file = sys.argv[2]
    full_report_file = sys.argv[3]
else:
    results_file = "/dev/null"
    summary_file = full_report_file = "/dev/stdout"

# Load board data and job URLs from environment variables
ALL_BOARD_DATA = json.loads(ALL_BOARD_DATA_STR)
ALL_BOARD_DATA = { b['board']: b for b in ALL_BOARD_DATA }

JOB_URLS = json.loads(WORKFLOW_JOBS_STR)
JOB_URLS = { j['name']: j['url'] for j in JOB_URLS }

# Import and process data for each board
for board_data in ALL_BOARD_DATA.values():
    # Extract common fields from board data
    artifact = board_data['artifact']
    board = board_data['board']
    variant = board_data['variant']
    subarch = board_data['subarch']

    # Get job link for this build
    # NOTE: This must match the name used in the workflow YAML
    job_link = JOB_URLS.get(f"Build for {board}")
    if job_link:
        job_link += "#step:5:1"

    # Read loader build data
    BOARD_LOADERS[board] = LoaderEntry(artifact, board, variant, job_link)
    if BOARD_LOADERS[board].status == FAILURE:
        log_test(artifact, board, 'CI test', '', [], FAILURE, "Core data could not be read.")
        continue

    # Get job link for this test
    job_link = JOB_URLS.get(f"Test {board}")
    if job_link:
        job_link += "#step:6:1"

    # Extract data from the report file
    report_file = f"arduino-{subarch}-{board}.json"
    if not os.path.exists(report_file):
        log_test(artifact, board, 'CI test', '', [], FAILURE, f"Report file not found.", job_link)
        continue # Skip to the next board

    try:
        with open(report_file, 'r') as f:
            report_data = json.load(f)
    except Exception as e:
        log_test(artifact, board, 'CI test', '', [], FAILURE, f"Error reading report file: {e}", job_link)
        continue # Skip to the next board

    reports = report_data.get('boards', [{}])[0].get('sketches', [])
    if not reports:
        log_test(artifact, board, 'CI test', '', [], FAILURE, "Test report is empty, check CI log.", job_link)
        continue # Skip to the next board

    # Iterate through individual sketch reports
    for report in reports:
        sketch = report.get('name', 'unknown_sketch')
        success = report.get('compilation_success', False)
        issues = report.get('issues', [])

        # Replace long absolute paths with '...' for brevity.
        sketch_issues = [ re.sub(r'(/.+?)((/[^/]+){3}):', r'...\2:', issue) for issue in issues ]

        if not success:
            status = ERROR
        elif len(sketch_issues): # Implies warnings/non-critical issues
            status = WARNING
        else:
            status = PASS

        log_test(artifact, board, sketch, status, sketch_issues, job_link)

ARTIFACTS = sorted(ARTIFACT_TESTS.keys())

# Begin output of the report
# --------------------------

ci_run_status = max(res.status for res in ARTIFACT_TESTS.values())
ci_run_passed = ci_run_status in (PASS, WARNING)

# update the results file
with open(results_file, 'w') as f:
    f.write('Tests PASSED.' if ci_run_passed else '::error::Tests FAILED.');
    f.write(f" See {JOB_LINK_STEM}#user-content-summary for details.\n")

# Write the summary to the summary file
with open(summary_file, 'w') as f:
    f_print = lambda *args, **kwargs: print(*args, file=f, **kwargs)

    print_summary()

# Write the detailed information to the full report file
with open(full_report_file, 'w') as f:
    f_print = lambda *args, **kwargs: print(*args, file=f, **kwargs)

    # Print the test matrix sections, per artifact
    for artifact in ARTIFACTS:
        artifact_boards = sorted([ board for board in ARTIFACT_TESTS[artifact].boards if BOARD_TESTS[board].status != FAILURE ])

        if not artifact_boards:
            continue

        f_print(f"<a name='{artifact}'></a>")
        f_print("\n---\n")
        print_test_matrix(artifact, artifact_boards, "issues", sketch_filter=lambda res: res.status == ERROR)

        # print successful tests matrix in a collapsible section
        successful_tests = ARTIFACT_TESTS[artifact].counts[PASS] + ARTIFACT_TESTS[artifact].counts[WARNING]
        warning_tests = ARTIFACT_TESTS[artifact].counts[WARNING]
        if successful_tests:
            summary = f"{successful_tests} successful <code>{artifact}</code> tests hidden"
            if warning_tests:
                summary += f" ({warning_tests} with warnings)"

            f_print(f"<details><summary>{summary}</summary><blockquote><br>\n")
            print_test_matrix(artifact, artifact_boards, "tests", sketch_filter=lambda res: res.status in (PASS, WARNING))
            f_print("</blockquote></details>\n")
